#!/usr/bin/python
from Bio import SeqIO
from Bio.SeqRecord import SeqRecord
from Bio.Alphabet import IUPAC
from Bio.SeqFeature import SeqFeature, FeatureLocation
from Bio import Restriction
from Bio.Restriction import *
from Bio.Seq import Seq
from Bio.Alphabet import generic_dna, generic_protein
from Bio.SeqUtils import MeltingTemp as mt
from subprocess import call
from StringIO import StringIO
import os
import sys

# v2working for cluster #5.

TRIM_SIZE = 1800  # maximum allowable length of a gene for ordering from Twist
OVERHANG_LEN = 60
OPERON_SIZE = 6000


# From Colin's script, modified by GV
# Use this function to chop up sequences and add name to each chopped fragment
def split_seq(this_seq, seq_name, trim_size, oh_length):
    seq = str(this_seq)
    split_seqs = {}
    seq_len = len(seq)
    # if the whole sequence length is smaller than the trim size, return the same sequence
    if seq_len < trim_size:
        split_seqs[seq_name] = seq
    else:
        split_num = int(seq_len/(trim_size - 2 * oh_length))
        if seq_len % (split_num * (trim_size - 2 * oh_length)) > 0:
            split_num += 1
        lfrag = int(seq_len / split_num)
        split_seqs[seq_name] = seq[0: (lfrag + oh_length / 2)]
        if split_num > 2:
            for i in range(1, split_num - 1):
                this_start = i * lfrag - (oh_length / 2)
                this_end = (i + 1) * lfrag + (oh_length / 2)
                seq_name_new = seq_name + "-" + str(i)
                split_seqs[seq_name_new] = seq[this_start: this_end]
                seq_name_new = seq_name
        split_seqs[seq_name] = seq[(seq_len - lfrag - (oh_length / 2)): seq_len]
    return split_seqs


# Convert fasta file into genbank
def fasta_to_genbank(fasta_file_name, cluster_name):

    gbfile_list = []
    # in thefasta filename are the two constructs: pET and pRSF
    fasta_file = open(fasta_file_name, "rU")
    sequences = list(SeqIO.parse(fasta_file, "fasta"))
    # Asign generic_dna or generic_protein
    for seq in sequences:
        # print str(seq.seq)
        seq.seq.alphabet = generic_dna
        # pritnt seq
        gbfile_name = fasta_file_name + "_" + seq.id + ".gb"
        gbfile_list.append(gbfile_name)

        # Create a sequence
        # sequence_string = seq.seq
        # print sequence_string
        sequence_object = Seq(str(seq.seq), IUPAC.unambiguous_dna)

        # Create a record
        record = SeqRecord(sequence_object,
                           id='123456789',  # random accession number
                           name=cluster_name,
                           description='An example GenBank file generated by BioPython')

        # Add annotation
        if "pETDuet" in gbfile_name:
            backbone_name = "pETDuet"
            for element in pETDuet_construct_list:
                element_start = pETDuet_construct_seq_with_re.find(elements_to_seq[element])
                element_end = element_start + len(elements_to_seq[element])
                feature = SeqFeature(FeatureLocation(start=element_start, end=element_end), type='CDS', qualifiers={'locus_tag': element})
                print element, element_start, element_end
                record.features.append(feature)

        if "pRSFDuet" in gbfile_name:
            backbone_name = "pRSFDuet"
            for element in pRSFDuet_construct_list:
                element_start = pRSFDuet_construct_seq_with_re.find(elements_to_seq[element])
                element_end = element_start + len(elements_to_seq[element])
                feature = SeqFeature(FeatureLocation(start=element_start, end=element_end), type='CDS', qualifiers={'locus_tag':element})
                print element, element_start, element_end
                record.features.append(feature)
        # print record.features

        # Save as GenBank file
        gbfile_full = cluster_name + "-" + backbone_name + ".gb"
        output_file = open(gbfile_full, 'w')
        SeqIO.write(record, output_file, 'genbank')


elements_to_seq = {"T7_tetO": "taatacgactcactataggTCTATCATTGATAGGgtttccctctagata",
                   "UTRB": "tctagaaataattttgtttaactttaagaaggagatatacc",
                   "UTRA": "ataatttaaaaaacagacctcatatcgaaataaaagaaggagatatacc",
                   "UTRd1": "tgtagaaataattttgtttaactttaataaggagatatacc",
                   "UTRd2": "atcttagtatattagttaagtataagaaggagatatacata",
                   "insulator_wt": "agggctccggactgcgctgtatagt",
                   "insulator_m6": "cccgtacgcgagataaactgctagg",
                   "pheA1": "gacgaacaaTAAGGCCTCCCAAATCGGGGGGCCTTTTTTATTgaTaacaaaa",
                   "ECK120033737": "ggaaacacagAAAAAAGCCCGCACCTGACAGTGCGGGCTTTTTTTTTcgaccaaagg",
                   "ECK120029600": "TTCAGCCAAAAAACTTAAGACCGCCGGTCTTGTCCACTACCTTGCAGTAATGCGGTGGACAGGATCGGCGGTTTTCTTTTCTCTTCTCAA",
                   "NotI": "GCGGCCGC",
                   "SrfI": "GCCCGGGC",
                   "SwaI": "ATTTAAAT",
                   "PmlI": "CACGTG",
                   "PmeI": "GTTTAAAC",
                   "SalI": "GTCGAC",
                   "SexAI": "ACCAGGT",
                   "SgrAI": "CACCGGCG",
                   "NheI": "GCTAGC",
                   "HindIII": "AAGCTT",
                   "AscI": "GGCGCGCC",
                   "SpeI": "ACTAGT",
                   "ura3_extended_upoh": "tgtaagcggatgccgggagcagacaagcccgtcagggcgcgtcagcgggtgttggcgggt",
                   "ura3_extended_downoh": "TATTACCCTATGCGGTGTGAAATACCGCACAGATGCGTAAGGAGAAAATACCGCATCAGG"}
prom_list = ["T7_tetO"]
utr_list = ["UTRB", "UTRA", "UTRd1", "UTRd2"]
term_list = ["ECK120029600", "ECK120033737", "pheA1"]
ins_list = ["insulator_wt", "insulator_m6"]
re_list = ["NotI", "SrfI", "SwaI", "PmlI", "PmeI", "SalI", "SexAI", "SgrAI", "NheI", "HindIII", "AscI", "SpeI"]

# Input fasta file of backbones and ura3
backbones_list_file = sys.argv[1]
# backbone_name_to_seq = {}
backbone_list = []
for record in SeqIO.parse(backbones_list_file, "fasta"):
    seq_name = record.id
    seq = record.seq
    # add backbone sequences to dictionary with all genetic elements
    elements_to_seq[seq_name] = seq
    backbone_list.append(seq_name)

# Input fasta file of PKS orfs:
orf_list_file = sys.argv[2]
cluster_name = sys.argv[3]
total_cluster_len = 0
orf_list = []
name_to_seq = {}  # need this dict to calculate size of operon

for record in SeqIO.parse(orf_list_file, "fasta"):
    seq_name = record.id
    seq = record.seq
    seq_len = len(record.seq)
    total_cluster_len += seq_len
    print seq_name, seq_len, len(seq), seq[:10]
    elements_to_seq[seq_name] = seq
    name_to_seq[seq_name] = seq  # need this dict to calculate size of operon
    orf_list.append(seq_name)

num_genes = len(orf_list)
print "\nTotal clulster len", total_cluster_len
num_operons = (total_cluster_len / OPERON_SIZE) + 1
print "Number Operons %s" % num_operons

if num_operons > 4:
    print "increase size of operon"
    sys.exit(1)

orf_list.sort(key=lambda x: len(name_to_seq[x]), reverse=True)
print "Selected as first on each operon %s" % orf_list[0:num_operons]

used_orf = set()
for el in orf_list[0:num_operons]:
    used_orf.add(el)


def get_promotor_gen():
    index = 0
    while True:
        yield prom_list[index]
        index += 1
        index = index % len(prom_list)

promotor_gen = get_promotor_gen()


def get_term_gen():
    index = 0
    while True:
        yield term_list[index]
        index += 1
        index = index % len(term_list)

term_gen = get_term_gen()


def get_utr_gen():
    index = 0
    while True:
        yield utr_list[index]
        index += 1
        index = index % len(utr_list)

utr_gen = get_utr_gen()


def get_ins_gen():
    index = 0
    while True:
        yield ins_list[index]
        index += 1
        index = index % len(ins_list)

ins_gen = get_ins_gen()


def get_operon_size(operon):
    size = 0
    for o in operon:
        if o in name_to_seq:
            size += len(name_to_seq[o])
        elif o in elements_to_seq:
            size += len(elements_to_seq[o])
        # print o, size
    return size


def get_gene(size_left):
    for o in orf_list:
        if o in used_orf:
            continue
        if len(name_to_seq[o]) < size_left:
            return o
    return None


def construct_operon(initial_element):
    operon = []
    operon.append(next(promotor_gen))
    operon.append(next(utr_gen))
    operon.append(initial_element)
    operon_size_left = OPERON_SIZE - get_operon_size(operon)
    while operon_size_left > 0:
        # print operon_size_left
        next_gene = get_gene(operon_size_left)
        # print next_gene
        if not next_gene:
            break

        operon.append(next(utr_gen))
        operon.append(next_gene)
        used_orf.add(next_gene)
        operon_size_left = OPERON_SIZE - get_operon_size(operon)
    operon.append(next(term_gen))
    return operon


operons = []
for operon_id in xrange(num_operons):
    operon = construct_operon(orf_list[operon_id])
    operons.append(operon)
    print "Operon %s has size %s : %s" % (operon_id, get_operon_size(operon), operon)

plasmids = []
for i in xrange(0, num_operons, 2):
    plasmid = []
    plasmid.append(backbone_list[i / 2])
    plasmid += operons[i]
    plasmid.append(backbone_list[-1])
    plasmid.append(next(ins_gen))
    plasmid += operons[i + 1]
    print "Plasmid %s" % plasmid
    plasmids.append(plasmid)


def get_seq(plasmid):
    seq = ""
    for p in plasmid:
        seq += str(elements_to_seq[p])
    return seq

full_seq = ''.join(get_seq(plasmid) for plasmid in plasmids)

# Search for restriction enzymes in sequences and add them
allowed_re = []
for re in re_list:
    # print re, NotI.site  # not working
    re_seq = elements_to_seq[re].lower()
    construct_seq = full_seq.lower()
    print re, full_seq.find(re_seq), "in the whole sequence"
    if full_seq.find(re_seq) == -1:
        allowed_re.append(re)

if len(allowed_re) < 4:
    print "Add more enzymes to search for in the sequence"
allowed_re = sorted(allowed_re)
print sorted(allowed_re)

re_index = 0

# Store plasmid sequences in list
plasmid_seqs = []

for plasmid_id, plasmid in enumerate(plasmids):
    print "plasmid id", plasmid_id
    re_index = 0
    new_plasmid = []
    for item in plasmid:
        if item in prom_list or item in term_list:
            new_plasmid.append(allowed_re[re_index])
            re_index += 1
        new_plasmid.append(item)
        if item in prom_list:
            new_plasmid.append(allowed_re[re_index])

    plasmids[plasmid_id] = new_plasmid
    print new_plasmid

    # Concatenate all seqeuce elements into two pET and pRSF constructs
    plasmid_seqs.append(get_seq(plasmid))

print "pET construct len: ", len(plasmid_seqs[0])
print "pRSF construct len: ", len(plasmid_seqs[1])

pETDuet_construct_seq_with_re = str(plasmid_seqs[0])
pRSFDuet_construct_seq_with_re = str(plasmid_seqs[1])


# sys.exit(1)

# ########## Step # Write all sequences in files

# Write in files
cluster_dir = cluster_name
if not os.path.exists(cluster_dir):
    os.makedirs(cluster_dir)

# Write seq of two plasmids in a file
with open(os.path.join(cluster_dir, cluster_name + "_constructs.out.fasta"),"w") as f:
    # with open("constructs.out.fasta", "w") as f:
    f.write(">pETDuet_c\n{}\n>pRSFDuet_c\n{}\n".format(pETDuet_construct_seq_with_re, pRSFDuet_construct_seq_with_re))
f.close()

# Split sequence into Full fragments
start_index_ura = pETDuet_construct_seq_with_re.find(str(elements_to_seq['ura3_extended']))
fragment_1 = pETDuet_construct_seq_with_re[:start_index_ura + OVERHANG_LEN]
end_index_ura = start_index_ura + len(elements_to_seq['ura3_extended']) - OVERHANG_LEN
fragment_2 = pETDuet_construct_seq_with_re[end_index_ura:]
start_index_ura = pRSFDuet_construct_seq_with_re.find(str(elements_to_seq['ura3_extended']))
fragment_3 = pRSFDuet_construct_seq_with_re[:start_index_ura + OVERHANG_LEN]
end_index_ura = start_index_ura + len(elements_to_seq['ura3_extended']) - OVERHANG_LEN
fragment_4 = pRSFDuet_construct_seq_with_re[end_index_ura:]

# Put full fragments in a dictionary
fragment_name_1 = cluster_name + "_" + "Fr1"
fragment_name_2 = cluster_name + "_" + "Fr2"
fragment_name_3 = cluster_name + "_" + "Fr3"
fragment_name_4 = cluster_name + "_" + "Fr4"
full_fragments = {fragment_name_1: fragment_1, fragment_name_2: fragment_2,
                  fragment_name_3: fragment_3, fragment_name_4: fragment_4}

print "fragment 1", fragment_1[:20], fragment_1[-60:]
print "fragment 2", fragment_2[:120], fragment_2[-20:]
print "fragment 3", fragment_3[:20], fragment_3[-60:]
print "fragment 4", fragment_4[:120], fragment_4[-20:]

# Write full fragments in a file
with open(os.path.join(cluster_dir, cluster_name + "_full_fragments.out.fasta"),"w") as f:
    f.write(">{}\n{}".format(fragment_name_1, fragment_1))
    f.write("\n>{}\n{}".format(fragment_name_2, fragment_2))
    f.write("\n>{}\n{}".format(fragment_name_3, fragment_3))
    f.write("\n>{}\n{}".format(fragment_name_4, fragment_4))
    f.close()

w = open(os.path.join(cluster_dir, cluster_name + "_twist_fragments.out.fasta"), "w")
# Split Full fragments into twist fragments
for fragment_name in full_fragments.keys():
    twist_fragments = split_seq(full_fragments[fragment_name], fragment_name, TRIM_SIZE, OVERHANG_LEN)
    # print twist_fragments
    with open(os.path.join(cluster_dir, cluster_name + "_twist_fragments.out.fasta"), "a") as f:
        for fragment in twist_fragments.keys():
            f.write(">{}\n{}\n".format(fragment, twist_fragments[fragment]))

fasta_to_genbank((os.path.join(cluster_dir, cluster_name + "_constructs.out.fasta")), cluster_name)
